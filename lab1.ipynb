{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8vtuqXEBP1j"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habijanmarija/lab1/blob/main/lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Lab 1 - Introduction to Jupyter notebooks, Google Colab, Basic Image Manipulation, and Color Spaces\n",
        "\n",
        "These laboratory excersises are solved on Google Colab and are save on GitHub repo that is connected to GitHub Classroom.\n",
        "\n",
        "## Tools You need to use to Submit Assignments\n",
        "\n",
        "In this document, you will solve tasks. This is a Jupyter Notebook which has the **.ipynb** extension, is an interactive web environment for data analysis, visualization, solution presentations, education, and more.\n",
        "\n",
        "**Google Colab** is a tool that allows you to run and share Jupyter Notebook files on Google's servers, including the use of Google's CPU, GPU, and TPU resources. Colab is like Google Docs for Jupyter Notebooks. **Google Colab does not automatically save your assignment to GitHub.**\n",
        "\n",
        "**You use GitHub to save and submit your assignments.** When you accept the assignment through GitHub Classroom, a repository is automatically created on your GitHub account with a copy of the task. This is where you will save your solutions. Saving your solutions submits the tasks for that lab.\n",
        "\n",
        "## How to Solve the Tasks?\n",
        "1. Accept the task via the Google Classroom link that you will receive. Google Classroom will create a repository on your account.\n",
        "2. Go to the newly created repository on your account and click on the .ipynb file, then click Open in Colab.\n",
        "3. You will solve the tasks in Google Colab.\n",
        "\n",
        "## How to Save (Submit) Tasks?\n",
        "\n",
        "1. In Google Colab, click on the Open settings gear icon in the top-right corner.\n",
        "2. Click on the GitHub tab and check the box for Access private repositories and organizations.\n",
        "3. A new window will open for you to grant access to GitHub. For ferit-osirv, click Grant.\n",
        "4. Save and exit the settings.\n",
        "5. Click on File > Save a copy in GitHub.\n",
        "6. Select the lab repository that includes your name.\n",
        "\n",
        "> *Note:* You only need to complete steps 1-4 the first time.\n",
        "\n",
        "7. Click on **File > Save a copy in GitHub**.\n",
        "8. Select created repository **that has your name in it**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlt9F0--QgIC"
      },
      "source": [
        "# Introduction to Jupyter Notebook\n",
        "\n",
        "A Jupyter Notebook consists of two types of cells: text and code. You are currently reading a text cell, which can contain text, markdown, or HTML. It is useful for adding more information about your code, allowing the notebook to be read as a narrative, which is where the name Jupyter Notebook comes from.\n",
        "\n",
        "Code cells contain Python code or command-line commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "GjI2lX1EPUqK",
        "outputId": "805dd757-013b-4ed3-c0dc-a2cebb501066"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Vaš kod pišete u kodne blokove kao što je ovaj'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"You write your code in code block like this one\"\n",
        "# These code cells will always display the result of the last line of code below the cell.\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZe57cDBRCE7"
      },
      "source": [
        "You can execute code cells by selecting them with your mouse and pressing the play button in the upper left corner of the cell. You can also run the cell and move to the next one with the shortcut **Shift+Enter**. **Control+Enter** will execute the current cell and keep it selected.\n",
        "\n",
        "The output of code cells appears below the cell. It can include outputs from ,print statements, as well as graphs and images generated using libraries like OpenCV or matplotlib.\n",
        "\n",
        "Additionally, the result of the last line of code is always displayed at the end of the output.\n",
        "\n",
        "Feel free to experiment with the code cells and modify this notebook however you like!\n",
        "\n",
        "**For more tips** check out the following notebook: https://colab.research.google.com/notebooks/basic_features_overview.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5DM9UScViXU"
      },
      "source": [
        "## Order of execution\n",
        "\n",
        "It's important to remember that cells are executed in the order in which you run them. For example, take a look at the following 3 cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYQ36HP5VwOX"
      },
      "outputs": [],
      "source": [
        "a = 3 # Block 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhVqiFS2VxDp"
      },
      "outputs": [],
      "source": [
        "a = 2 # Block 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPhu-8R5VxpF",
        "outputId": "8203f75e-148c-4372-e2c6-342eada0692c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a # Block 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmBrMmukV1aG"
      },
      "source": [
        "Try running cell 2 first, then cell 1, and finally cell 3. You will notice that the output of cell 3 changes because cell 1 was executed and a was assigned a new value. This behavior can lead to bugs in your code. Therefore, it is good practice to periodically, and especially before submission, run all cells in order again. You can do this by clicking ***Runtime > Run** all or pressing F9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pVjDTjqWayF"
      },
      "source": [
        "## Importing libraries\n",
        "\n",
        "For completing lab exercises, you'll need various libraries such as numpy, OpenCV, matplotlib, etc. It's good practice to have a separate cell for importing all the required libraries at the beginning of the notebook. Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXS_YJC2WsWD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iB2rlmOWyZb"
      },
      "source": [
        "This way, you ensure that the cell is always executed, and all the necessary libraries are loaded. Additionally, once these libraries are loaded, Google Colab will provide suggestions (auto-complete) as you type, which will significantly ease your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feUPz7IDCbDx"
      },
      "source": [
        "## Copying Files from the GitHub Repository\n",
        "\n",
        "For completing the exercises, you will need images and other files that will be stored in the GitHub repository of the exercise. A command like this will be available in the notebook for each exercise. It will copy the files from GitHub to the Google Colab environment.\n",
        "\n",
        "**You need to run this command before starting each exercise.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpP_i0KgCefb",
        "outputId": "90975ec0-b6f9-470c-d341-5f20be691f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'clone'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/29)\u001b[K\rremote: Counting objects:   6% (2/29)\u001b[K\rremote: Counting objects:  10% (3/29)\u001b[K\rremote: Counting objects:  13% (4/29)\u001b[K\rremote: Counting objects:  17% (5/29)\u001b[K\rremote: Counting objects:  20% (6/29)\u001b[K\rremote: Counting objects:  24% (7/29)\u001b[K\rremote: Counting objects:  27% (8/29)\u001b[K\rremote: Counting objects:  31% (9/29)\u001b[K\rremote: Counting objects:  34% (10/29)\u001b[K\rremote: Counting objects:  37% (11/29)\u001b[K\rremote: Counting objects:  41% (12/29)\u001b[K\rremote: Counting objects:  44% (13/29)\u001b[K\rremote: Counting objects:  48% (14/29)\u001b[K\rremote: Counting objects:  51% (15/29)\u001b[K\rremote: Counting objects:  55% (16/29)\u001b[K\rremote: Counting objects:  58% (17/29)\u001b[K\rremote: Counting objects:  62% (18/29)\u001b[K\rremote: Counting objects:  65% (19/29)\u001b[K\rremote: Counting objects:  68% (20/29)\u001b[K\rremote: Counting objects:  72% (21/29)\u001b[K\rremote: Counting objects:  75% (22/29)\u001b[K\rremote: Counting objects:  79% (23/29)\u001b[K\rremote: Counting objects:  82% (24/29)\u001b[K\rremote: Counting objects:  86% (25/29)\u001b[K\rremote: Counting objects:  89% (26/29)\u001b[K\rremote: Counting objects:  93% (27/29)\u001b[K\rremote: Counting objects:  96% (28/29)\u001b[K\rremote: Counting objects: 100% (29/29)\u001b[K\rremote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 29 (delta 3), reused 26 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (29/29), 5.27 MiB | 22.78 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf clone && git clone https://github.com/pui-sum-rv/lab1 clone && cp -a clone/. ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPg8Vf9Cr8D"
      },
      "source": [
        "**Google Colab will occasionally delete all files**. Therefore, you might need to rerun this command between sessions. If you encounter errors indicating that files do not exist, try running the command again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgvfxpurQ1VG"
      },
      "source": [
        "# Digital Image Processing vs. Computer Vision\n",
        "\n",
        "To begin with, Computer Graphics creates a mathematical or geometric model from an original captured image to produce a digital image. However, it's important not to confuse this with Digital Image Processing or the field of Computer Vision, even though they complement each other. Digital Image Processing results in a modified version of the image, whereas Computer Vision provides a meaningful interpretation of the image using Artificial Intelligence techniques.\n",
        "\n",
        "Nevertheless, to sucessfully deal with Computer Vision, we need to know some stuff from Image Processing field.\n",
        "\n",
        "## So, how we digitize an image?\n",
        "Simply put, the process of sampling (digitizing coordinate values of\n",
        ") + quantization (digitizing amplitude values\n",
        ") will generate an digital image\n",
        ", where the function represents intesity / gray level and the coordinate represents picture element (known as pixel).\n",
        "\n",
        "![digitizing](https://i.postimg.cc/G2r5cPg3/digitizing.png)\n",
        "\n",
        "Therefore, the digital image can be simply coded in a matrix form as:\n",
        "\n",
        "\n",
        "\\begin{bmatrix} f(0, 0) & f(0, 1) & ... & f(0, N-1) \\\\ f(1, 0) & f(1, 1) & ... & f(1, N-1) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ f(M-1, 0) & f(M-1, 1) & ... & f(M-1, N-1) \\end{bmatrix}\n",
        "\n",
        "where M is number of rows and N is number of columns. Since we'll use Python, the pixel starts from (0,0) and ends in (M-1, N-1).\n",
        "\n",
        "## Bits Required for Digital Image Storage\n",
        "\n",
        "Number of gray levels ($L$) allowed in each pixel formulated as $L = 2^k \\iff k = \\frac{ln(L)}{ln(2)}$, where $k$ denotes a bit (binary digit). Therefore, a ***k-bit image*** requires $N \\times M \\times k$ bits to store a digitized image. If in case $M == N$ (height and weight of pixel are the same), its formula can be simplified as $N^2 \\times k$. For example, an image with\n",
        "**L = 256 possible gray-level values** is called an $2^k = 2^8 = $ **8-bit** image; and if this 8-bit image has 32x32 dimension thus it'll need $32 \\times 32 \\times 8 = 8,912$ bit or equivalent to **1,024 byte = 1 KB** (remember: 8 bit = 1 byte and 1,024 byte = 1 kilobyte / KB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDfWMU5AhzEL"
      },
      "source": [
        "## Image Aquisition and Image Loading\n",
        "\n",
        "Image acquisition process can be as simple as image capturing (e.g. with camera), loading, and saving.\n",
        "\n",
        "TO BE NOTICED\n",
        "* **matplotlib.pyplot** represents **<font color=\"red\">R</font><font color=\"limegreen\">G</font><font color=\"blue\">B</font>** image\n",
        "* **cv2** represents **<font color=\"blue\">B</font><font color=\"limegreen\">G</font><font color=\"red\">R</font>** image (reversed)\n",
        "\n",
        "Therefore, if you want to use **cv.imread()** to load an image and **plt.imshow()** to show it, convert the image first.\n",
        "<br>Otherwise, the use of **cv2.imshow()** will open a new window to show up an image (rather than directly showed on your IPYNB cell)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFVn1CpKizbB"
      },
      "source": [
        "First lets import necesary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from imutils import resize\n",
        "from numpy.random import uniform as un\n",
        "from numpy import float32, uint8, array, arange, ones, amax, sqrt, concatenate, pi, cos, sin\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyDJ_R_OiV-E"
      },
      "source": [
        "Lets load an image. First, we will write the function for image loading and showing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PaA9ZatfikIh"
      },
      "outputs": [],
      "source": [
        "def plot_image(img, title=None, font_size=None, axis=\"off\", color=cv2.COLOR_BGR2RGB): # since CV2 uses BGR image, convert BGR to RGB\n",
        "    plt.title(title, fontsize=font_size) # plot figure name/title\n",
        "    plt.axis(axis) # \"off\" == remove pyplot axes\n",
        "\n",
        "    \"\"\"\n",
        "    You DO NOT NEED the if-else configuration below IF you're using OPENCV v4.0. or above.\n",
        "    Simply do plt.imshow(cv2.cvtColor(img, color)) for either BGR or grayscale image.\n",
        "    \"\"\"\n",
        "\n",
        "    if len(img.shape) == 3: # means it has 3-channels color\n",
        "        plt.imshow(cv2.cvtColor(img, color))\n",
        "    else: # for 1-channel image\n",
        "        plt.imshow(img, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VykEnZ7bi9pO"
      },
      "source": [
        "After that we will call that function and use it to read an image, convert it to grayscale image and then we will convert grayscale image to binary image (image that contains only black and white pixels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G7rDnb45jVMd"
      },
      "outputs": [],
      "source": [
        "# Original Image\n",
        "img = cv2.imread(\"slike/airplane.bmp\") # cv2.imread(\"path\", cv2.IMREAD_COLOR(default)|cv2.IMREAD_GRAYSCALE(0)|cv2.IMREAD_UNCHANGED(-1))\n",
        "\n",
        "# Grayscale Image\n",
        "img_grayscaled = cv2.imread(\"slike/airplane.bmp\", 0) # or cv2.imread(\"img/lena.png\". cv2.IMREAD_GRAYSCALE)\n",
        "# img_grayscaled = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Binary Image (MUST BE from Grayscale Image)\n",
        "_, img_binary = cv2.threshold(img_grayscaled, 127, 255, cv2.THRESH_BINARY) # grayscale (NOT RGB) -> binary image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWNq83USkVTR"
      },
      "source": [
        "Now lets show those images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "5I6utHuFkbdh",
        "outputId": "63036962-38df-4a99-c6f7-657e9b348f13"
      },
      "outputs": [],
      "source": [
        "origin_fig_names = [\n",
        "    \"RGB Image\\nshape={}\".format(img.shape),\n",
        "    \"Grayscale Image\\nshape={}\".format(img_grayscaled.shape),\n",
        "    \"Binary Image\\nshape={}\".format(img_binary.shape)\n",
        "] # will be set as title for each image\n",
        "\n",
        "origin_fig = plt.figure(figsize=(13, 5)) # figsize(width/horizontally, height/vertically)\n",
        "origin_img = [img, img_grayscaled, img_binary]\n",
        "\n",
        "for i in range(len(origin_img)):\n",
        "    x = origin_fig.add_subplot(1, 3, i+1) # position index always starts from 1, thus i+1\n",
        "    plot_image(origin_img[i], title=origin_fig_names[i])\n",
        "plt.tight_layout() # margin adjusted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITu8oxpemG3t"
      },
      "source": [
        "And finally, lets write them to directory: /content/slike"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcVjb-o2mN85"
      },
      "outputs": [],
      "source": [
        "cv2.imwrite(\"img/lena.png\", img) # cv2.imwrite(\"path/name.ext\", your_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRuk6Bc8PgF8"
      },
      "source": [
        "# Tasks about Image Manipulation\n",
        " \n",
        "In these exercises, you will familiarize yourself with the basics of image manipulation in Python. \n",
        "\n",
        "## Tasks\n",
        "\n",
        "Solve the following tasks within this Jupyter Notebook, ensuring that the result of each task is displayed below the respective code cell. **Add at least one code cell for each task.**\n",
        "\n",
        "\n",
        "1. Write a program that loads an image and displays three separate images, one for each channel. (Hint: When displaying one channel, set the other two channels to 0)\n",
        "\n",
        "2. Write a program that loads an image and creates a border of 10 pixels around the image. Display the resulting image.\n",
        "\n",
        "3. Write a program that loads an image and creates three new images from it. The first should have half the vertical pixels (rows) of the original, the second should have half the horizontal pixels (columns), and the third should have half of both. (Hint: take every other pixel). Display all three images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Color spaces\n",
        "\n",
        "In this lab, you'll get familiar with image color spaces. On the web and in general usage, most images are encoded as **RGB**: **R**ed, **G**reen, and **B**lue. OpenCV generally uses **BGR**: Blue, Green, Red.\n",
        "\n",
        "This is just one of the many ways we can represent an image. In an RGB image, we get a pixel by mixing the three colors. We can get the same pixel by using different numbers and formulae to combine them. For instance, the **CMYK** color space encodes each pixel in 4 primary colors: **C**yan, **M**agenta, **Y**ellow and **K**ey (Black). Since printers use these primary colors, CMYK is often used when preparing images for print.\n",
        "\n",
        "Not all color spaces consist only of primary colors. For instance, **HSV** (**H**ue, **S**aturation, **V**alue) stores the color in Hue, the color's intensity in Saturation, and the general brightness of that pixel in Value. The Hue portion is a number in [0, 179] (in OpenCV, usually it's an arc around a circle, so [0, 360)) where 0 is red, and the hue slowly shifts to green and then blue as you get to higher numbers.\n",
        "\n",
        "![hsv](https://i.postimg.cc/XqdVJn2Y/rgb-to-hsv.jpg)\n",
        "\n",
        "You can think of the whole HSV color space as a cylinder. The height on the cylinder corresponds to how dark the pixel is, the distance from the center tells you how non-gray it is, and the angle tells you which color the pixel is.\n",
        "\n",
        "There are many color spaces each with its uses. One other color space we'll mention in **YCbCr**. Y is the **luma** component, similar to the Value in HSV. Cb is the **blue-difference chroma component**, i.e. how blue should this pixel be tinted. Similarly, Cr is the **red-difference chroma component**, which tells you how much should a pixel be tinted red. Even with a different type of representation, each YCbCr is capable of showing all RGB images.\n",
        "\n",
        "![ycbc](https://i.postimg.cc/w38tBfgD/rgb-to-ycrcb.png)\n",
        "\n",
        "The reason YCbCr is important is because of the human eye. Our eyes are much more sensitive to luminance than actual color differences. Therefore, when compressing images, it's better to compress the chroma components than luminance if you want the image to look the same to a human observer.  This is called **chroma subsampling** and is used heavily in image and video compression, including MPEG, JPEG, DVD and Blu-Rays, and many others.\n",
        "\n",
        "## In OpenCV\n",
        "\n",
        "OpenCV supports a plethora of color spaces for images. The main function to convert color spaces is: [img = cv.cvtColor(img, code)](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab). The `code` tells OpenCV **from** which format to convert the image, as well as **to** which format. You can see all the color conversion codes [here](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#ga4e0972be5de079fed4e3a10e24ef5ef0). For example:\n",
        "\n",
        "- cv.COLOR_BGR2YCrCb (BGR to YCbCr)\n",
        "- cv.COLOR_YCrCb2BGR (back to BGR as the name suggests)\n",
        "- cv.COLOR_RGB2HSV\n",
        "- etc.\n",
        "\n",
        "Note: You'll have to convert the image back to RGB if you want to use matplotlib to display it in its original form. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tasks about Color spaces\n",
        "In these exercises, you will familiarize yourself with the basics of different color spaces. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1\n",
        "\n",
        "Load the image `images/peppers.png` using OpenCV and convert it to the HSV color space using the aforementioned function. Then, add 30 to the H (hue) channel of the HSV image for each pixel. Convert that image back to RGB and display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2\n",
        "\n",
        "Load the image `images/peppers.png` using OpenCV and convert it to the HSV color space using the aforementioned function. Then, set the H (hue) channel of the HSV image to 0 for each pixel. Convert that image back to RGB and display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
